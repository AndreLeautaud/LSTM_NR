{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# why dont these python scripts auto save in the directory..? \n",
    "    # Do I need to sync them or something? \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n2o_nn/LSTM_June2022/2_matlab_traintest_matdata/XTrain_base_SL30_n2osf05.mat\n",
      "/n2o_nn/LSTM_June2022/4_Python_LSTMmodels_hdf5/base_SL30_n2osf05_DO20_w_not_norm.hdf5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.backend import square, mean\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    " \n",
    "import datetime\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "#4 covariate model with rain, n20, wfps, [and n2osf]\n",
    "    # SL = 15, 30, 60  day sequences \n",
    "    # n2osf = 1, 3, 5, 7, 15 day sequences \n",
    "    # DO = 0.2,0.3,0.4   (0.5?)\n",
    "    \n",
    "# set some [LSTM model] variables \n",
    "SL=\"30\"\n",
    "n2osf=\"05\"\n",
    "DO=\"20\"\n",
    "\n",
    "n2osfI= int(n2osf)\n",
    "SLI= int(SL)\n",
    "DOI= int(DO)/100 \n",
    "\n",
    "filenameX='/n2o_nn/LSTM_June2022/2_matlab_traintest_matdata/XTrain_base_SL'+SL+\"_n2osf\"+n2osf+\".mat\"\n",
    "filenameY='/n2o_nn/LSTM_June2022/2_matlab_traintest_matdata/YTrain_base_SL'+SL+\"_n2osf\"+n2osf+\".mat\"\n",
    "filenameW='/n2o_nn/LSTM_June2022/2_matlab_traintest_matdata/Weights_base_SL'+SL+\"_n2osf\"+n2osf+\".mat\"\n",
    "print(filenameX)\n",
    "\n",
    "# load the mat data files     \n",
    "matx = loadmat(filenameX) \n",
    "maty = loadmat(filenameY)\n",
    "matw = loadmat(filenameW)\n",
    "\n",
    "# hdfname='/n2o_nn/LSTM_June2022/4_Python_LSTMmodels_hdf5/base_SL'+SL+\"_n2osf\"+n2osf+\"_DO\"+DO+\"_w_not_norm.hdf5\"\n",
    "hdfname='\\Users\\cddorich\\4_Python_LSTMmodels_hdf5base_SL'+SL+\"_n2osf\"+n2osf+\"_DO\"+DO+\"_w_not_norm.hdf5'\n",
    "\n",
    "print(hdfname)\n",
    "# potential other covariates that could be added:\n",
    "# learningrate = lr001\n",
    "# epoch = epoch500\n",
    "# rmsprop = rmsprop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72180\n",
      "StandardScaler()\n",
      "[11.56079198]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2406, 30, 4), (2406, 30, 1))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data prep for LSTM \n",
    "\n",
    "matx.keys()\n",
    "maty.keys()\n",
    "matx\n",
    "dfx = pd.DataFrame(np.hstack((matx['XTrain1'])))\n",
    "dfx.head\n",
    "dfy = pd.DataFrame(np.hstack((maty['YTrain1'])))\n",
    "dfy.head\n",
    "dfw = pd.DataFrame(np.hstack((matw['Weights'])))\n",
    "dfw.head\n",
    "\n",
    "arrx = dfx.values\n",
    "arry = dfy.values\n",
    "arrw = dfw.values\n",
    "resultx = arrx[:,0]\n",
    "resulty = arry[:,0]\n",
    "resultw = arrw[:,0]\n",
    " \n",
    "xlist = []\n",
    "ylist  = []\n",
    "wlist  = []\n",
    "\n",
    "for i in range(len(resultx)):\n",
    "    xlist.append(np.transpose(resultx[i]))\n",
    "    ylist.append(np.transpose(resulty[i]))\n",
    "    \n",
    "Xtrain = np.asarray(xlist)\n",
    "Xtrain \n",
    "\n",
    "#print( len(Xtrain) )\n",
    "#print( len(xlist) )\n",
    "\n",
    "Xtrain = np.asarray(xlist)\n",
    "\n",
    "nvals=len(xlist)*SLI\n",
    "print(nvals)\n",
    "ynvals=len(ylist)*SLI\n",
    "\n",
    "# this value is the rows/cells of the Xtrain * the SL, and # of covariates\n",
    "Xtrain = np.reshape(Xtrain, (nvals, 4)) \n",
    "dfx = pd.DataFrame(Xtrain)\n",
    "\n",
    "Ytrain = np.asarray(ylist).reshape(ynvals,1) \n",
    "dfy = pd.DataFrame(Ytrain)\n",
    "\n",
    "dfx.columns = ['sampn2o', 'rainseq','wfpsseq','n2o']\n",
    "df_targets = pd.DataFrame(dfy)\n",
    "df_targets.columns = ['target n2o']\n",
    "\n",
    "x_data = dfx.values\n",
    "y_data = df_targets.values\n",
    "num_data = len(x_data)\n",
    "num_data \n",
    "\n",
    "#new training and testing for 30 day sequences and with lesser hidden neurons. Comment it out to go back to the original 60 day sequence batches\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(y_data))\n",
    "print(scaler.mean_)\n",
    "y_data =scaler.transform(y_data)\n",
    "y_data.shape\n",
    "\n",
    "x_data = x_data.reshape((len(xlist),SLI,4))  #30 day 7 cov repeat after mistake , day since and fert applied + soil temp 10 with new hold out \n",
    "y_data = y_data.reshape((len(ylist),SLI,1))\n",
    "\n",
    "x_data.shape, y_data.shape\n",
    " \n",
    "    \n",
    "#try standarsizing the target variable as the model weights were exploding due to the large variation in the y  or the target data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2406"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2406,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AL 6-14-22 sample_weights\n",
    "wlist=resultw.astype(int)\n",
    "sample_weight=[]\n",
    "\n",
    "for i in wlist:\n",
    "    sample_weight+=[1/i]*i\n",
    "\n",
    "sample_weight=np.array(sample_weight)\n",
    "sample_weight.shape\n",
    "# sample_weight/=np.mean(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01818182, 0.01818182,\n",
       "       0.01818182, 0.01818182, 0.01818182, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.00393701, 0.00393701, 0.00393701, 0.00393701, 0.00393701,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 , 0.0212766 ,\n",
       "       0.0212766 , 0.0212766 , 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02      , 0.02      , 0.02      , 0.02      , 0.02      ,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.02325581, 0.02325581,\n",
       "       0.02325581, 0.02325581, 0.02325581, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.01754386, 0.01754386, 0.01754386, 0.01754386, 0.01754386,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.00934579, 0.00934579, 0.00934579,\n",
       "       0.00934579, 0.00934579, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138,\n",
       "       0.01724138, 0.01724138, 0.01724138, 0.01724138, 0.01724138])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #faster more complicated array version, not working\n",
    "# wlist=resultw.astype(int)\n",
    "# sample_weight=np.zeros(x_data.shape[0])\n",
    "# index=0\n",
    "# for i in wlist:\n",
    "#     sample_weight[index]=[1/i]*i\n",
    "#     index+=1\n",
    "\n",
    "# sample_weight=np.array(sample_weight)\n",
    "# sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2406"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM architecture here  \n",
    "    \n",
    "# notes/architecture to QC \n",
    "    # changing the architecture to reduce hidden layer neurons to prevent overfitting with 25 neurons,\n",
    "    # 50 precent dropout\n",
    "    #and 30 time steps as input sequences. \n",
    "    \n",
    "from keras.layers import Dropout\n",
    "\n",
    "# from tensorflow.keras import layers \n",
    "\n",
    "model = Sequential()\n",
    "# model = keras.Sequential() # tf2.3\n",
    "\n",
    "# is this needed...? \n",
    "# # Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# # output embedding dimension of size 64.\n",
    "# model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# for tensorflow 2.3\n",
    "# https://www.tensorflow.org/guide/keras/rnn\n",
    "# model.add(layers.LSTM(128,activation='tanh',\n",
    "#                return_sequences=True,\n",
    "#                ) )\n",
    "# need to figure out the input_shape parameter \n",
    "\n",
    "# print(model)\n",
    "\n",
    "# need to see about tensorflow syntax, AN used 1.15\n",
    "#model.add(LSTM(50,activation='tanh',\n",
    "#              return_sequences=True,  \n",
    "#             input_shape=(30,7)))\n",
    "\n",
    "model.add(LSTM(50,                  # what is this 50? layers I believe...\n",
    "             activation='tanh',      # activation function \n",
    "             return_sequences=True,   \n",
    "             input_shape=(SLI,4)))    # SL and covariates \n",
    "\n",
    "# this DO part needs to be vetted/tested ## \n",
    "  # if doing dropout 0.4 then need both 50 and 32 neuron \n",
    "    # otherwise dropout 0.2, 0.3 use just 50 layer \n",
    "  \n",
    "    # if DOI=0.4, then add this second layer code line:\n",
    "# model.add(LSTM(32, activation='tanh', return_sequences=True))\n",
    "\n",
    "model.add(Dropout(DOI))    # set dropout layers \n",
    "\n",
    "##model.add(Dense(num_y_signals, activation='linear'))\n",
    "model.add(Dense(1))\n",
    "# # model.add(Dense(1, activation='relu'))\n",
    " \n",
    " \n",
    " # dont think below is needed...?:\n",
    "# if False:\n",
    "#     from tensorflow.python.keras.initializers import RandomUniform\n",
    "\n",
    "#     # Maybe use lower init-ranges.\n",
    "#     init = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "\n",
    "#     model.add(Dense(num_y_signals,\n",
    "#                     activation='linear',\n",
    "#                     kernel_initializer=init))\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = adam(lr=1e-2)\n",
    "# optimizer\n",
    "# from keras.optimizers import adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "# opt = adam(lr=0.001)\n",
    "\n",
    "opt = RMSprop(lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 30, 50)            11000     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 50)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30, 1)             51        \n",
      "=================================================================\n",
      "Total params: 11,051\n",
      "Trainable params: 11,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "path_checkpoint = '23_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=20, verbose=1) #initially patience was 5 for the 60 sequence model, now let's make it 20 to give the model more attempts at convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./23_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    " callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=20,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [callback_early_stopping,\n",
    "#              callback_checkpoint,\n",
    "#              callback_tensorboard,\n",
    "#              callback_reduce_lr]\n",
    "\n",
    "x_data.shape\n",
    "y_data.shape\n",
    "\n",
    " \n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)\n",
    "\n",
    "# mcp_save = ModelCheckpoint(filepath = '/Users/abhijnannath/crossvalid_18_30day_mdlwts_4covariate_base_Do20_lr001_epoch500_rmsprop_new_monitorloss_aug5.hdf5', save_best_only=True, monitor='loss', mode='min')\n",
    "mcp_save = ModelCheckpoint(filepath = hdfname, save_best_only=True, monitor='loss', mode='min')\n",
    "# mcp_save = ModelCheckpoint(filepath = '/n2o_nn/30day_7covariate_Do20_lr001_epoch500_rmsprop_new_monitorloss_n2osf7.hdf5', save_best_only=True, monitor='loss', mode='min')\n",
    "\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_delta=1e-4, mode='min')\n",
    "callbacks = [mcp_save]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2165 samples, validate on 241 samples\n",
      "Epoch 1/500\n",
      "2165/2165 [==============================] - 2s 930us/step - loss: 0.0040 - val_loss: 7.1417e-04\n",
      "Epoch 2/500\n",
      "2165/2165 [==============================] - 1s 508us/step - loss: 0.0029 - val_loss: 6.1161e-04\n",
      "Epoch 3/500\n",
      "2165/2165 [==============================] - 1s 491us/step - loss: 0.0026 - val_loss: 6.3285e-04\n",
      "Epoch 4/500\n",
      "2165/2165 [==============================] - 1s 498us/step - loss: 0.0025 - val_loss: 5.7838e-04\n",
      "Epoch 5/500\n",
      "2165/2165 [==============================] - 1s 490us/step - loss: 0.0024 - val_loss: 6.0200e-04\n",
      "Epoch 6/500\n",
      "2165/2165 [==============================] - 1s 496us/step - loss: 0.0024 - val_loss: 5.8236e-04\n",
      "Epoch 7/500\n",
      "2165/2165 [==============================] - 1s 491us/step - loss: 0.0024 - val_loss: 6.0273e-04\n",
      "Epoch 8/500\n",
      "2165/2165 [==============================] - 1s 505us/step - loss: 0.0023 - val_loss: 5.7848e-04\n",
      "Epoch 9/500\n",
      "2165/2165 [==============================] - 1s 505us/step - loss: 0.0023 - val_loss: 5.6580e-04\n",
      "Epoch 10/500\n",
      "2165/2165 [==============================] - 1s 540us/step - loss: 0.0023 - val_loss: 5.6887e-04\n",
      "Epoch 11/500\n",
      "2165/2165 [==============================] - 1s 509us/step - loss: 0.0022 - val_loss: 5.6765e-04\n",
      "Epoch 12/500\n",
      "2165/2165 [==============================] - 1s 510us/step - loss: 0.0022 - val_loss: 6.0137e-04\n",
      "Epoch 13/500\n",
      "2165/2165 [==============================] - 1s 512us/step - loss: 0.0021 - val_loss: 5.7384e-04\n",
      "Epoch 14/500\n",
      "2165/2165 [==============================] - 1s 512us/step - loss: 0.0021 - val_loss: 5.6816e-04\n",
      "Epoch 15/500\n",
      "2165/2165 [==============================] - 1s 508us/step - loss: 0.0021 - val_loss: 5.7187e-04\n",
      "Epoch 16/500\n",
      "2165/2165 [==============================] - 1s 511us/step - loss: 0.0021 - val_loss: 5.9605e-04\n",
      "Epoch 17/500\n",
      "2165/2165 [==============================] - 1s 507us/step - loss: 0.0020 - val_loss: 5.6906e-04\n",
      "Epoch 18/500\n",
      "2165/2165 [==============================] - 1s 508us/step - loss: 0.0019 - val_loss: 5.8996e-04\n",
      "Epoch 19/500\n",
      "2165/2165 [==============================] - 1s 512us/step - loss: 0.0019 - val_loss: 5.7563e-04\n",
      "Epoch 20/500\n",
      "2165/2165 [==============================] - 1s 517us/step - loss: 0.0019 - val_loss: 5.6609e-04\n",
      "Epoch 21/500\n",
      "2165/2165 [==============================] - 1s 516us/step - loss: 0.0018 - val_loss: 5.9330e-04\n",
      "Epoch 22/500\n",
      "2165/2165 [==============================] - 1s 514us/step - loss: 0.0019 - val_loss: 5.8081e-04\n",
      "Epoch 23/500\n",
      "2165/2165 [==============================] - 1s 507us/step - loss: 0.0018 - val_loss: 5.8626e-04\n",
      "Epoch 24/500\n",
      "2165/2165 [==============================] - 1s 549us/step - loss: 0.0018 - val_loss: 5.8123e-04\n",
      "Epoch 25/500\n",
      "2165/2165 [==============================] - 1s 521us/step - loss: 0.0017 - val_loss: 5.9161e-04\n",
      "Epoch 26/500\n",
      "2165/2165 [==============================] - 1s 511us/step - loss: 0.0017 - val_loss: 5.8389e-04\n",
      "Epoch 27/500\n",
      "2165/2165 [==============================] - 1s 511us/step - loss: 0.0017 - val_loss: 6.0857e-04\n",
      "Epoch 28/500\n",
      "2165/2165 [==============================] - 1s 510us/step - loss: 0.0017 - val_loss: 5.8919e-04\n",
      "Epoch 29/500\n",
      "2165/2165 [==============================] - 1s 511us/step - loss: 0.0017 - val_loss: 5.8776e-04\n",
      "Epoch 30/500\n",
      "2165/2165 [==============================] - 1s 511us/step - loss: 0.0017 - val_loss: 5.8648e-04\n",
      "Epoch 31/500\n",
      "2165/2165 [==============================] - 1s 514us/step - loss: 0.0016 - val_loss: 5.9697e-04\n",
      "Epoch 32/500\n",
      "2165/2165 [==============================] - 1s 512us/step - loss: 0.0017 - val_loss: 5.9014e-04\n",
      "Epoch 33/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0016 - val_loss: 5.8083e-04\n",
      "Epoch 34/500\n",
      "2165/2165 [==============================] - 1s 526us/step - loss: 0.0016 - val_loss: 5.9834e-04\n",
      "Epoch 35/500\n",
      "2165/2165 [==============================] - 1s 516us/step - loss: 0.0016 - val_loss: 5.9676e-04\n",
      "Epoch 36/500\n",
      "2165/2165 [==============================] - 1s 510us/step - loss: 0.0016 - val_loss: 5.8350e-04\n",
      "Epoch 37/500\n",
      "2165/2165 [==============================] - 1s 515us/step - loss: 0.0016 - val_loss: 5.8362e-04\n",
      "Epoch 38/500\n",
      "2165/2165 [==============================] - 1s 544us/step - loss: 0.0015 - val_loss: 5.8130e-04\n",
      "Epoch 39/500\n",
      "2165/2165 [==============================] - 1s 515us/step - loss: 0.0015 - val_loss: 5.7481e-04\n",
      "Epoch 40/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0015 - val_loss: 6.0620e-04\n",
      "Epoch 41/500\n",
      "2165/2165 [==============================] - 1s 515us/step - loss: 0.0015 - val_loss: 5.8212e-04\n",
      "Epoch 42/500\n",
      "2165/2165 [==============================] - 1s 512us/step - loss: 0.0015 - val_loss: 5.8778e-04\n",
      "Epoch 43/500\n",
      "2165/2165 [==============================] - 1s 513us/step - loss: 0.0015 - val_loss: 5.9961e-04\n",
      "Epoch 44/500\n",
      "2165/2165 [==============================] - 1s 515us/step - loss: 0.0015 - val_loss: 6.0459e-04\n",
      "Epoch 45/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0015 - val_loss: 5.8525e-04\n",
      "Epoch 46/500\n",
      "2165/2165 [==============================] - 1s 509us/step - loss: 0.0015 - val_loss: 5.8034e-04\n",
      "Epoch 47/500\n",
      "2165/2165 [==============================] - 1s 527us/step - loss: 0.0014 - val_loss: 6.0040e-04\n",
      "Epoch 48/500\n",
      "2165/2165 [==============================] - 1s 529us/step - loss: 0.0014 - val_loss: 5.9355e-04\n",
      "Epoch 49/500\n",
      "2165/2165 [==============================] - 1s 524us/step - loss: 0.0014 - val_loss: 5.8071e-04\n",
      "Epoch 50/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0014 - val_loss: 6.0158e-04\n",
      "Epoch 51/500\n",
      "2165/2165 [==============================] - 1s 524us/step - loss: 0.0014 - val_loss: 5.9827e-04\n",
      "Epoch 52/500\n",
      "2165/2165 [==============================] - 1s 570us/step - loss: 0.0014 - val_loss: 6.0665e-04\n",
      "Epoch 53/500\n",
      "2165/2165 [==============================] - 1s 530us/step - loss: 0.0014 - val_loss: 5.8244e-04\n",
      "Epoch 54/500\n",
      "2165/2165 [==============================] - 1s 519us/step - loss: 0.0014 - val_loss: 6.3122e-04\n",
      "Epoch 55/500\n",
      "2165/2165 [==============================] - 1s 520us/step - loss: 0.0014 - val_loss: 6.3664e-04\n",
      "Epoch 56/500\n",
      "2165/2165 [==============================] - 1s 521us/step - loss: 0.0013 - val_loss: 5.9024e-04\n",
      "Epoch 57/500\n",
      "2165/2165 [==============================] - 1s 516us/step - loss: 0.0013 - val_loss: 5.9919e-04\n",
      "Epoch 58/500\n",
      "2165/2165 [==============================] - 1s 517us/step - loss: 0.0014 - val_loss: 6.1206e-04\n",
      "Epoch 59/500\n",
      "2165/2165 [==============================] - 1s 524us/step - loss: 0.0013 - val_loss: 6.1345e-04\n",
      "Epoch 60/500\n",
      "2165/2165 [==============================] - 1s 517us/step - loss: 0.0013 - val_loss: 5.8214e-04\n",
      "Epoch 61/500\n",
      "2165/2165 [==============================] - 1s 519us/step - loss: 0.0012 - val_loss: 5.8586e-04\n",
      "Epoch 62/500\n",
      "2165/2165 [==============================] - 1s 516us/step - loss: 0.0013 - val_loss: 5.9508e-04\n",
      "Epoch 63/500\n",
      "2165/2165 [==============================] - 1s 523us/step - loss: 0.0012 - val_loss: 6.0358e-04\n",
      "Epoch 64/500\n",
      "2165/2165 [==============================] - 1s 521us/step - loss: 0.0012 - val_loss: 6.2975e-04\n",
      "Epoch 65/500\n",
      "2165/2165 [==============================] - 1s 521us/step - loss: 0.0012 - val_loss: 6.2069e-04\n",
      "Epoch 66/500\n",
      "2165/2165 [==============================] - 1s 553us/step - loss: 0.0012 - val_loss: 5.7249e-04\n",
      "Epoch 67/500\n",
      "2165/2165 [==============================] - 1s 516us/step - loss: 0.0013 - val_loss: 5.8221e-04\n",
      "Epoch 68/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0012 - val_loss: 6.4358e-04\n",
      "Epoch 69/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0012 - val_loss: 5.7163e-04\n",
      "Epoch 70/500\n",
      "2165/2165 [==============================] - 1s 521us/step - loss: 0.0012 - val_loss: 5.8613e-04\n",
      "Epoch 71/500\n",
      "2165/2165 [==============================] - 1s 524us/step - loss: 0.0012 - val_loss: 5.9636e-04\n",
      "Epoch 72/500\n",
      "2165/2165 [==============================] - 1s 532us/step - loss: 0.0012 - val_loss: 5.8428e-04\n",
      "Epoch 73/500\n",
      "2165/2165 [==============================] - 1s 523us/step - loss: 0.0011 - val_loss: 5.7043e-04\n",
      "Epoch 74/500\n",
      "2165/2165 [==============================] - 1s 519us/step - loss: 0.0011 - val_loss: 6.1716e-04\n",
      "Epoch 75/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165/2165 [==============================] - 1s 521us/step - loss: 0.0011 - val_loss: 6.5159e-04\n",
      "Epoch 76/500\n",
      "2165/2165 [==============================] - 1s 524us/step - loss: 0.0011 - val_loss: 6.5129e-04\n",
      "Epoch 77/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0011 - val_loss: 5.9897e-04\n",
      "Epoch 78/500\n",
      "2165/2165 [==============================] - 1s 518us/step - loss: 0.0011 - val_loss: 6.0104e-04\n",
      "Epoch 79/500\n",
      "2165/2165 [==============================] - 1s 522us/step - loss: 0.0011 - val_loss: 5.9100e-04\n",
      "Epoch 80/500\n",
      "2165/2165 [==============================] - 1s 592us/step - loss: 0.0010 - val_loss: 5.9061e-04\n",
      "Epoch 81/500\n",
      "2165/2165 [==============================] - 1s 561us/step - loss: 0.0011 - val_loss: 6.0750e-04\n",
      "Epoch 82/500\n",
      "2165/2165 [==============================] - 1s 557us/step - loss: 0.0010 - val_loss: 6.2362e-04\n",
      "Epoch 83/500\n",
      "2165/2165 [==============================] - 1s 559us/step - loss: 0.0010 - val_loss: 6.6288e-04\n",
      "Epoch 84/500\n",
      "2165/2165 [==============================] - 1s 560us/step - loss: 9.9526e-04 - val_loss: 6.3077e-04\n",
      "Epoch 85/500\n",
      "2165/2165 [==============================] - 1s 562us/step - loss: 9.7769e-04 - val_loss: 5.8547e-04\n",
      "Epoch 86/500\n",
      "2165/2165 [==============================] - 1s 566us/step - loss: 9.9058e-04 - val_loss: 6.2287e-04\n",
      "Epoch 87/500\n",
      "2165/2165 [==============================] - 1s 584us/step - loss: 0.0010 - val_loss: 6.2130e-04\n",
      "Epoch 88/500\n",
      "2165/2165 [==============================] - 1s 580us/step - loss: 9.6264e-04 - val_loss: 6.2749e-04\n",
      "Epoch 89/500\n",
      "2165/2165 [==============================] - 1s 581us/step - loss: 9.0785e-04 - val_loss: 6.2382e-04\n",
      "Epoch 90/500\n",
      "2165/2165 [==============================] - 1s 587us/step - loss: 9.8140e-04 - val_loss: 6.3373e-04\n",
      "Epoch 91/500\n",
      "2165/2165 [==============================] - 1s 591us/step - loss: 9.4778e-04 - val_loss: 6.1641e-04\n",
      "Epoch 92/500\n",
      "2165/2165 [==============================] - 1s 592us/step - loss: 9.2389e-04 - val_loss: 6.4850e-04\n",
      "Epoch 93/500\n",
      "2165/2165 [==============================] - 1s 604us/step - loss: 9.6602e-04 - val_loss: 5.8428e-04\n",
      "Epoch 94/500\n",
      "2165/2165 [==============================] - 1s 573us/step - loss: 9.1334e-04 - val_loss: 6.6108e-04\n",
      "Epoch 95/500\n",
      "2165/2165 [==============================] - 1s 603us/step - loss: 9.3635e-04 - val_loss: 6.2107e-04\n",
      "Epoch 96/500\n",
      "2165/2165 [==============================] - 1s 602us/step - loss: 8.8331e-04 - val_loss: 6.0731e-04\n",
      "Epoch 97/500\n",
      "2165/2165 [==============================] - 1s 601us/step - loss: 8.7931e-04 - val_loss: 6.1212e-04\n",
      "Epoch 98/500\n",
      "2165/2165 [==============================] - 1s 606us/step - loss: 9.0046e-04 - val_loss: 6.3817e-04\n",
      "Epoch 99/500\n",
      "2165/2165 [==============================] - 1s 602us/step - loss: 8.7409e-04 - val_loss: 6.5217e-04\n",
      "Epoch 100/500\n",
      "2165/2165 [==============================] - 1s 602us/step - loss: 8.4432e-04 - val_loss: 6.1532e-04\n",
      "Epoch 101/500\n",
      "2165/2165 [==============================] - 1s 603us/step - loss: 8.5709e-04 - val_loss: 6.5118e-04\n",
      "Epoch 102/500\n",
      "2165/2165 [==============================] - 1s 601us/step - loss: 8.4397e-04 - val_loss: 5.9313e-04\n",
      "Epoch 103/500\n",
      "2165/2165 [==============================] - 1s 601us/step - loss: 8.3100e-04 - val_loss: 6.3308e-04\n",
      "Epoch 104/500\n",
      "2165/2165 [==============================] - 1s 603us/step - loss: 8.4763e-04 - val_loss: 6.4060e-04\n",
      "Epoch 105/500\n",
      "2165/2165 [==============================] - 1s 625us/step - loss: 7.9314e-04 - val_loss: 6.5866e-04\n",
      "Epoch 106/500\n",
      "2165/2165 [==============================] - 1s 610us/step - loss: 8.1353e-04 - val_loss: 6.1842e-04\n",
      "Epoch 107/500\n",
      "2165/2165 [==============================] - 1s 619us/step - loss: 7.9848e-04 - val_loss: 6.0301e-04\n",
      "Epoch 108/500\n",
      "2165/2165 [==============================] - 1s 628us/step - loss: 8.0117e-04 - val_loss: 6.3220e-04\n",
      "Epoch 109/500\n",
      "2165/2165 [==============================] - 1s 626us/step - loss: 7.7605e-04 - val_loss: 6.0319e-04\n",
      "Epoch 110/500\n",
      "2165/2165 [==============================] - 1s 605us/step - loss: 7.8790e-04 - val_loss: 6.0319e-04\n",
      "Epoch 111/500\n",
      "2165/2165 [==============================] - 1s 602us/step - loss: 8.2502e-04 - val_loss: 6.5919e-04\n",
      "Epoch 112/500\n",
      "2165/2165 [==============================] - 1s 611us/step - loss: 7.7770e-04 - val_loss: 6.2497e-04\n",
      "Epoch 113/500\n",
      "2165/2165 [==============================] - 1s 604us/step - loss: 7.8244e-04 - val_loss: 6.0766e-04\n",
      "Epoch 114/500\n",
      "2165/2165 [==============================] - 1s 597us/step - loss: 7.7883e-04 - val_loss: 6.1427e-04\n",
      "Epoch 115/500\n",
      "2165/2165 [==============================] - 1s 606us/step - loss: 8.0260e-04 - val_loss: 5.9193e-04\n",
      "Epoch 116/500\n",
      "2165/2165 [==============================] - 1s 599us/step - loss: 7.5677e-04 - val_loss: 6.1464e-04\n",
      "Epoch 117/500\n",
      "2165/2165 [==============================] - 1s 627us/step - loss: 7.9668e-04 - val_loss: 6.1222e-04\n",
      "Epoch 118/500\n",
      "2165/2165 [==============================] - 1s 609us/step - loss: 7.6096e-04 - val_loss: 5.7424e-04\n",
      "Epoch 119/500\n",
      "2165/2165 [==============================] - 1s 597us/step - loss: 7.2657e-04 - val_loss: 6.3438e-04\n",
      "Epoch 120/500\n",
      "2165/2165 [==============================] - 1s 608us/step - loss: 7.4347e-04 - val_loss: 6.4096e-04\n",
      "Epoch 121/500\n",
      "2165/2165 [==============================] - 1s 614us/step - loss: 7.6032e-04 - val_loss: 6.1953e-04\n",
      "Epoch 122/500\n",
      "2165/2165 [==============================] - 1s 617us/step - loss: 7.2192e-04 - val_loss: 5.9550e-04\n",
      "Epoch 123/500\n",
      "2165/2165 [==============================] - 1s 618us/step - loss: 7.1976e-04 - val_loss: 5.9361e-04\n",
      "Epoch 124/500\n",
      "2165/2165 [==============================] - 1s 605us/step - loss: 7.5203e-04 - val_loss: 5.7681e-04\n",
      "Epoch 125/500\n",
      "2165/2165 [==============================] - 1s 620us/step - loss: 7.2492e-04 - val_loss: 5.9101e-04\n",
      "Epoch 126/500\n",
      "2165/2165 [==============================] - 1s 616us/step - loss: 7.4391e-04 - val_loss: 6.1092e-04\n",
      "Epoch 127/500\n",
      "2165/2165 [==============================] - 1s 610us/step - loss: 7.4215e-04 - val_loss: 6.3891e-04\n",
      "Epoch 128/500\n",
      "2165/2165 [==============================] - 1s 622us/step - loss: 7.2240e-04 - val_loss: 6.5130e-04\n",
      "Epoch 129/500\n",
      "2165/2165 [==============================] - 1s 648us/step - loss: 7.0447e-04 - val_loss: 6.1005e-04\n",
      "Epoch 130/500\n",
      "2165/2165 [==============================] - 1s 621us/step - loss: 7.1811e-04 - val_loss: 6.2084e-04\n",
      "Epoch 131/500\n",
      "2165/2165 [==============================] - 1s 628us/step - loss: 6.9118e-04 - val_loss: 5.7913e-04\n",
      "Epoch 132/500\n",
      "2165/2165 [==============================] - 1s 618us/step - loss: 7.1330e-04 - val_loss: 6.2562e-04\n",
      "Epoch 133/500\n",
      "2165/2165 [==============================] - 1s 623us/step - loss: 6.7319e-04 - val_loss: 6.3369e-04\n",
      "Epoch 134/500\n",
      "2165/2165 [==============================] - 1s 630us/step - loss: 6.8373e-04 - val_loss: 6.1599e-04\n",
      "Epoch 135/500\n",
      "2165/2165 [==============================] - 1s 634us/step - loss: 7.1607e-04 - val_loss: 6.4167e-04\n",
      "Epoch 136/500\n",
      "2165/2165 [==============================] - 1s 642us/step - loss: 6.9855e-04 - val_loss: 6.4998e-04\n",
      "Epoch 137/500\n",
      "2165/2165 [==============================] - 1s 649us/step - loss: 6.7258e-04 - val_loss: 5.9328e-04\n",
      "Epoch 138/500\n",
      "2165/2165 [==============================] - 1s 641us/step - loss: 6.7546e-04 - val_loss: 6.9315e-04\n",
      "Epoch 139/500\n",
      "2165/2165 [==============================] - 1s 639us/step - loss: 6.6793e-04 - val_loss: 5.9665e-04\n",
      "Epoch 140/500\n",
      "2165/2165 [==============================] - 1s 643us/step - loss: 6.8028e-04 - val_loss: 6.2352e-04\n",
      "Epoch 141/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 6.6945e-04 - val_loss: 6.2865e-04\n",
      "Epoch 142/500\n",
      "2165/2165 [==============================] - 1s 641us/step - loss: 6.7784e-04 - val_loss: 6.1174e-04\n",
      "Epoch 143/500\n",
      "2165/2165 [==============================] - 1s 639us/step - loss: 6.7605e-04 - val_loss: 6.6030e-04\n",
      "Epoch 144/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 6.4424e-04 - val_loss: 6.2600e-04\n",
      "Epoch 145/500\n",
      "2165/2165 [==============================] - 1s 648us/step - loss: 6.9506e-04 - val_loss: 6.1860e-04\n",
      "Epoch 146/500\n",
      "2165/2165 [==============================] - 1s 655us/step - loss: 6.5096e-04 - val_loss: 6.0234e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/500\n",
      "2165/2165 [==============================] - 1s 643us/step - loss: 6.3269e-04 - val_loss: 6.3972e-04\n",
      "Epoch 148/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 6.6457e-04 - val_loss: 6.3568e-04\n",
      "Epoch 149/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 6.1044e-04 - val_loss: 6.1443e-04\n",
      "Epoch 150/500\n",
      "2165/2165 [==============================] - 1s 643us/step - loss: 6.1840e-04 - val_loss: 6.4753e-04\n",
      "Epoch 151/500\n",
      "2165/2165 [==============================] - 1s 684us/step - loss: 6.3574e-04 - val_loss: 6.2476e-04\n",
      "Epoch 152/500\n",
      "2165/2165 [==============================] - 2s 724us/step - loss: 6.1776e-04 - val_loss: 6.4519e-04\n",
      "Epoch 153/500\n",
      "2165/2165 [==============================] - 1s 683us/step - loss: 6.3593e-04 - val_loss: 6.1795e-04\n",
      "Epoch 154/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 6.2720e-04 - val_loss: 6.3892e-04\n",
      "Epoch 155/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 6.5965e-04 - val_loss: 6.5903e-04\n",
      "Epoch 156/500\n",
      "2165/2165 [==============================] - 1s 648us/step - loss: 6.1051e-04 - val_loss: 6.2775e-04\n",
      "Epoch 157/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 6.2711e-04 - val_loss: 6.0788e-04\n",
      "Epoch 158/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 6.0935e-04 - val_loss: 6.2601e-04\n",
      "Epoch 159/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 6.0671e-04 - val_loss: 6.3425e-04\n",
      "Epoch 160/500\n",
      "2165/2165 [==============================] - 1s 692us/step - loss: 6.2891e-04 - val_loss: 6.0756e-04\n",
      "Epoch 161/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 5.8604e-04 - val_loss: 6.2660e-04\n",
      "Epoch 162/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 5.9693e-04 - val_loss: 6.1893e-04\n",
      "Epoch 163/500\n",
      "2165/2165 [==============================] - 2s 718us/step - loss: 5.9814e-04 - val_loss: 6.3614e-04\n",
      "Epoch 164/500\n",
      "2165/2165 [==============================] - 1s 688us/step - loss: 6.3748e-04 - val_loss: 6.4042e-04\n",
      "Epoch 165/500\n",
      "2165/2165 [==============================] - 1s 688us/step - loss: 5.7291e-04 - val_loss: 6.2888e-04\n",
      "Epoch 166/500\n",
      "2165/2165 [==============================] - 2s 694us/step - loss: 5.7314e-04 - val_loss: 6.0922e-04\n",
      "Epoch 167/500\n",
      "2165/2165 [==============================] - 1s 688us/step - loss: 5.9901e-04 - val_loss: 6.2036e-04\n",
      "Epoch 168/500\n",
      "2165/2165 [==============================] - 1s 683us/step - loss: 5.9805e-04 - val_loss: 6.1415e-04\n",
      "Epoch 169/500\n",
      "2165/2165 [==============================] - 1s 682us/step - loss: 5.8591e-04 - val_loss: 6.2063e-04\n",
      "Epoch 170/500\n",
      "2165/2165 [==============================] - 2s 699us/step - loss: 5.8044e-04 - val_loss: 6.1761e-04\n",
      "Epoch 171/500\n",
      "2165/2165 [==============================] - 1s 686us/step - loss: 5.9809e-04 - val_loss: 6.1509e-04\n",
      "Epoch 172/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 5.6753e-04 - val_loss: 6.2907e-04\n",
      "Epoch 173/500\n",
      "2165/2165 [==============================] - 1s 686us/step - loss: 5.7450e-04 - val_loss: 6.1381e-04\n",
      "Epoch 174/500\n",
      "2165/2165 [==============================] - 2s 704us/step - loss: 5.4736e-04 - val_loss: 6.1936e-04\n",
      "Epoch 175/500\n",
      "2165/2165 [==============================] - 1s 660us/step - loss: 5.8045e-04 - val_loss: 6.0817e-04\n",
      "Epoch 176/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 5.6628e-04 - val_loss: 6.1217e-04\n",
      "Epoch 177/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 5.6983e-04 - val_loss: 5.9132e-04\n",
      "Epoch 178/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 5.8563e-04 - val_loss: 6.0639e-04\n",
      "Epoch 179/500\n",
      "2165/2165 [==============================] - 1s 673us/step - loss: 5.6471e-04 - val_loss: 5.9781e-04\n",
      "Epoch 180/500\n",
      "2165/2165 [==============================] - 1s 676us/step - loss: 5.5643e-04 - val_loss: 6.1127e-04\n",
      "Epoch 181/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 5.3890e-04 - val_loss: 5.8515e-04\n",
      "Epoch 182/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 5.4155e-04 - val_loss: 6.4065e-04\n",
      "Epoch 183/500\n",
      "2165/2165 [==============================] - 1s 691us/step - loss: 5.5885e-04 - val_loss: 5.9351e-04\n",
      "Epoch 184/500\n",
      "2165/2165 [==============================] - 2s 732us/step - loss: 5.2996e-04 - val_loss: 5.9525e-04\n",
      "Epoch 185/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 5.3101e-04 - val_loss: 5.8984e-04\n",
      "Epoch 186/500\n",
      "2165/2165 [==============================] - 1s 680us/step - loss: 5.0974e-04 - val_loss: 6.0008e-04\n",
      "Epoch 187/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 5.3628e-04 - val_loss: 5.9897e-04\n",
      "Epoch 188/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 5.2419e-04 - val_loss: 6.1286e-04\n",
      "Epoch 189/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 5.3084e-04 - val_loss: 6.3139e-04\n",
      "Epoch 190/500\n",
      "2165/2165 [==============================] - 1s 684us/step - loss: 5.3085e-04 - val_loss: 6.3094e-04\n",
      "Epoch 191/500\n",
      "2165/2165 [==============================] - 1s 691us/step - loss: 5.1934e-04 - val_loss: 6.3946e-04\n",
      "Epoch 192/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 5.0877e-04 - val_loss: 6.3769e-04\n",
      "Epoch 193/500\n",
      "2165/2165 [==============================] - 1s 682us/step - loss: 5.2101e-04 - val_loss: 6.4161e-04\n",
      "Epoch 194/500\n",
      "2165/2165 [==============================] - 2s 706us/step - loss: 5.2131e-04 - val_loss: 6.2984e-04\n",
      "Epoch 195/500\n",
      "2165/2165 [==============================] - 2s 724us/step - loss: 5.3338e-04 - val_loss: 6.3010e-04\n",
      "Epoch 196/500\n",
      "2165/2165 [==============================] - 2s 696us/step - loss: 5.2147e-04 - val_loss: 6.0173e-04\n",
      "Epoch 197/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 4.9962e-04 - val_loss: 6.6607e-04\n",
      "Epoch 198/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 5.0805e-04 - val_loss: 6.2578e-04\n",
      "Epoch 199/500\n",
      "2165/2165 [==============================] - 1s 682us/step - loss: 5.0518e-04 - val_loss: 6.0935e-04\n",
      "Epoch 200/500\n",
      "2165/2165 [==============================] - 1s 679us/step - loss: 4.9724e-04 - val_loss: 6.2938e-04\n",
      "Epoch 201/500\n",
      "2165/2165 [==============================] - 2s 712us/step - loss: 5.3639e-04 - val_loss: 6.2644e-04\n",
      "Epoch 202/500\n",
      "2165/2165 [==============================] - 2s 696us/step - loss: 5.0862e-04 - val_loss: 6.1418e-04\n",
      "Epoch 203/500\n",
      "2165/2165 [==============================] - 1s 679us/step - loss: 5.0612e-04 - val_loss: 6.1793e-04\n",
      "Epoch 204/500\n",
      "2165/2165 [==============================] - 1s 690us/step - loss: 4.9131e-04 - val_loss: 6.1302e-04\n",
      "Epoch 205/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 4.8600e-04 - val_loss: 6.5794e-04\n",
      "Epoch 206/500\n",
      "2165/2165 [==============================] - 2s 732us/step - loss: 5.0881e-04 - val_loss: 6.6034e-04\n",
      "Epoch 207/500\n",
      "2165/2165 [==============================] - 1s 680us/step - loss: 4.8380e-04 - val_loss: 6.1092e-04\n",
      "Epoch 208/500\n",
      "2165/2165 [==============================] - 1s 679us/step - loss: 4.9880e-04 - val_loss: 6.2267e-04\n",
      "Epoch 209/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 4.8264e-04 - val_loss: 6.2492e-04\n",
      "Epoch 210/500\n",
      "2165/2165 [==============================] - 1s 682us/step - loss: 4.9814e-04 - val_loss: 6.1987e-04\n",
      "Epoch 211/500\n",
      "2165/2165 [==============================] - 1s 691us/step - loss: 4.8351e-04 - val_loss: 6.1753e-04\n",
      "Epoch 212/500\n",
      "2165/2165 [==============================] - 2s 693us/step - loss: 4.6610e-04 - val_loss: 6.3417e-04\n",
      "Epoch 213/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 4.9055e-04 - val_loss: 6.1057e-04\n",
      "Epoch 214/500\n",
      "2165/2165 [==============================] - 2s 695us/step - loss: 4.8589e-04 - val_loss: 6.4775e-04\n",
      "Epoch 215/500\n",
      "2165/2165 [==============================] - 1s 679us/step - loss: 4.9759e-04 - val_loss: 6.4435e-04\n",
      "Epoch 216/500\n",
      "2165/2165 [==============================] - 2s 700us/step - loss: 4.7694e-04 - val_loss: 6.5264e-04\n",
      "Epoch 217/500\n",
      "2165/2165 [==============================] - 2s 699us/step - loss: 5.0726e-04 - val_loss: 6.2080e-04\n",
      "Epoch 218/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165/2165 [==============================] - 2s 698us/step - loss: 4.7308e-04 - val_loss: 6.3211e-04\n",
      "Epoch 219/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 4.7459e-04 - val_loss: 6.3808e-04\n",
      "Epoch 220/500\n",
      "2165/2165 [==============================] - 1s 692us/step - loss: 4.8556e-04 - val_loss: 6.0319e-04\n",
      "Epoch 221/500\n",
      "2165/2165 [==============================] - 1s 683us/step - loss: 4.8481e-04 - val_loss: 6.1098e-04\n",
      "Epoch 222/500\n",
      "2165/2165 [==============================] - 1s 681us/step - loss: 4.9692e-04 - val_loss: 6.1257e-04\n",
      "Epoch 223/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 4.5799e-04 - val_loss: 6.2462e-04\n",
      "Epoch 224/500\n",
      "2165/2165 [==============================] - 1s 674us/step - loss: 4.6472e-04 - val_loss: 6.2430e-04\n",
      "Epoch 225/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 4.4832e-04 - val_loss: 6.0877e-04\n",
      "Epoch 226/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 4.6459e-04 - val_loss: 6.3960e-04\n",
      "Epoch 227/500\n",
      "2165/2165 [==============================] - 2s 708us/step - loss: 4.5195e-04 - val_loss: 5.8665e-04\n",
      "Epoch 228/500\n",
      "2165/2165 [==============================] - 1s 681us/step - loss: 4.5999e-04 - val_loss: 5.9708e-04\n",
      "Epoch 229/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 4.6463e-04 - val_loss: 5.9760e-04\n",
      "Epoch 230/500\n",
      "2165/2165 [==============================] - 1s 676us/step - loss: 4.5916e-04 - val_loss: 6.0347e-04\n",
      "Epoch 231/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 4.6262e-04 - val_loss: 6.0703e-04\n",
      "Epoch 232/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 4.6653e-04 - val_loss: 6.1910e-04\n",
      "Epoch 233/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 4.5593e-04 - val_loss: 5.9753e-04\n",
      "Epoch 234/500\n",
      "2165/2165 [==============================] - 2s 699us/step - loss: 4.5591e-04 - val_loss: 6.5668e-04\n",
      "Epoch 235/500\n",
      "2165/2165 [==============================] - 1s 682us/step - loss: 4.4902e-04 - val_loss: 6.0442e-04\n",
      "Epoch 236/500\n",
      "2165/2165 [==============================] - 2s 700us/step - loss: 4.3673e-04 - val_loss: 5.8612e-04\n",
      "Epoch 237/500\n",
      "2165/2165 [==============================] - 1s 683us/step - loss: 4.3786e-04 - val_loss: 5.7937e-04\n",
      "Epoch 238/500\n",
      "2165/2165 [==============================] - 2s 725us/step - loss: 4.4481e-04 - val_loss: 5.9979e-04\n",
      "Epoch 239/500\n",
      "2165/2165 [==============================] - 1s 675us/step - loss: 4.5393e-04 - val_loss: 6.0776e-04\n",
      "Epoch 240/500\n",
      "2165/2165 [==============================] - 1s 684us/step - loss: 4.5398e-04 - val_loss: 6.2788e-04\n",
      "Epoch 241/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 4.3043e-04 - val_loss: 6.1395e-04\n",
      "Epoch 242/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 4.4804e-04 - val_loss: 6.1407e-04\n",
      "Epoch 243/500\n",
      "2165/2165 [==============================] - 1s 676us/step - loss: 4.3992e-04 - val_loss: 6.5323e-04\n",
      "Epoch 244/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 4.3907e-04 - val_loss: 6.0625e-04\n",
      "Epoch 245/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 4.3215e-04 - val_loss: 6.1453e-04\n",
      "Epoch 246/500\n",
      "2165/2165 [==============================] - 2s 693us/step - loss: 4.3514e-04 - val_loss: 6.4922e-04\n",
      "Epoch 247/500\n",
      "2165/2165 [==============================] - 1s 680us/step - loss: 4.3948e-04 - val_loss: 6.3470e-04\n",
      "Epoch 248/500\n",
      "2165/2165 [==============================] - 1s 679us/step - loss: 4.4639e-04 - val_loss: 6.0182e-04\n",
      "Epoch 249/500\n",
      "2165/2165 [==============================] - 2s 715us/step - loss: 4.4580e-04 - val_loss: 6.1639e-04\n",
      "Epoch 250/500\n",
      "2165/2165 [==============================] - 1s 686us/step - loss: 4.3198e-04 - val_loss: 6.3174e-04\n",
      "Epoch 251/500\n",
      "2165/2165 [==============================] - 1s 677us/step - loss: 4.2462e-04 - val_loss: 6.2027e-04\n",
      "Epoch 252/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 4.2263e-04 - val_loss: 6.3758e-04\n",
      "Epoch 253/500\n",
      "2165/2165 [==============================] - 1s 658us/step - loss: 4.2947e-04 - val_loss: 5.8410e-04\n",
      "Epoch 254/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 4.3242e-04 - val_loss: 6.3587e-04\n",
      "Epoch 255/500\n",
      "2165/2165 [==============================] - 1s 681us/step - loss: 4.1498e-04 - val_loss: 6.0150e-04\n",
      "Epoch 256/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 4.2279e-04 - val_loss: 6.0941e-04\n",
      "Epoch 257/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 4.2101e-04 - val_loss: 5.9816e-04\n",
      "Epoch 258/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 4.1736e-04 - val_loss: 6.1035e-04\n",
      "Epoch 259/500\n",
      "2165/2165 [==============================] - 1s 676us/step - loss: 4.0930e-04 - val_loss: 6.1513e-04\n",
      "Epoch 260/500\n",
      "2165/2165 [==============================] - 2s 705us/step - loss: 4.3524e-04 - val_loss: 6.0198e-04\n",
      "Epoch 261/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 4.1975e-04 - val_loss: 6.3004e-04\n",
      "Epoch 262/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 4.2277e-04 - val_loss: 6.0266e-04\n",
      "Epoch 263/500\n",
      "2165/2165 [==============================] - 1s 673us/step - loss: 4.1566e-04 - val_loss: 6.1501e-04\n",
      "Epoch 264/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 4.0985e-04 - val_loss: 5.9479e-04\n",
      "Epoch 265/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 4.3152e-04 - val_loss: 6.2669e-04\n",
      "Epoch 266/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 4.1241e-04 - val_loss: 6.4800e-04\n",
      "Epoch 267/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 4.1195e-04 - val_loss: 6.3553e-04\n",
      "Epoch 268/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 4.1152e-04 - val_loss: 6.1090e-04\n",
      "Epoch 269/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 4.1357e-04 - val_loss: 6.2388e-04\n",
      "Epoch 270/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 4.0217e-04 - val_loss: 6.1296e-04\n",
      "Epoch 271/500\n",
      "2165/2165 [==============================] - 2s 701us/step - loss: 4.0865e-04 - val_loss: 5.9857e-04\n",
      "Epoch 272/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 4.1057e-04 - val_loss: 6.1637e-04\n",
      "Epoch 273/500\n",
      "2165/2165 [==============================] - 1s 692us/step - loss: 3.9288e-04 - val_loss: 6.0726e-04\n",
      "Epoch 274/500\n",
      "2165/2165 [==============================] - 1s 659us/step - loss: 4.0842e-04 - val_loss: 6.0509e-04\n",
      "Epoch 275/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 4.0801e-04 - val_loss: 6.2986e-04\n",
      "Epoch 276/500\n",
      "2165/2165 [==============================] - 1s 651us/step - loss: 4.2367e-04 - val_loss: 6.1115e-04\n",
      "Epoch 277/500\n",
      "2165/2165 [==============================] - 1s 658us/step - loss: 3.8841e-04 - val_loss: 6.0838e-04\n",
      "Epoch 278/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 4.1855e-04 - val_loss: 6.0997e-04\n",
      "Epoch 279/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.8575e-04 - val_loss: 6.0146e-04\n",
      "Epoch 280/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 4.0633e-04 - val_loss: 6.1442e-04\n",
      "Epoch 281/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.9114e-04 - val_loss: 6.2875e-04\n",
      "Epoch 282/500\n",
      "2165/2165 [==============================] - 2s 698us/step - loss: 4.1847e-04 - val_loss: 6.2623e-04\n",
      "Epoch 283/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.7956e-04 - val_loss: 6.1730e-04\n",
      "Epoch 284/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 3.9394e-04 - val_loss: 6.3312e-04\n",
      "Epoch 285/500\n",
      "2165/2165 [==============================] - 1s 677us/step - loss: 3.9269e-04 - val_loss: 5.9392e-04\n",
      "Epoch 286/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 3.7823e-04 - val_loss: 6.2591e-04\n",
      "Epoch 287/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.8957e-04 - val_loss: 6.2024e-04\n",
      "Epoch 288/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 3.9835e-04 - val_loss: 6.3303e-04\n",
      "Epoch 289/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165/2165 [==============================] - 1s 660us/step - loss: 3.8890e-04 - val_loss: 6.1149e-04\n",
      "Epoch 290/500\n",
      "2165/2165 [==============================] - 1s 659us/step - loss: 3.8309e-04 - val_loss: 6.2703e-04\n",
      "Epoch 291/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 3.9157e-04 - val_loss: 6.0909e-04\n",
      "Epoch 292/500\n",
      "2165/2165 [==============================] - 1s 655us/step - loss: 4.0354e-04 - val_loss: 6.2632e-04\n",
      "Epoch 293/500\n",
      "2165/2165 [==============================] - 2s 703us/step - loss: 3.8301e-04 - val_loss: 6.1631e-04\n",
      "Epoch 294/500\n",
      "2165/2165 [==============================] - 1s 660us/step - loss: 3.8587e-04 - val_loss: 6.2274e-04\n",
      "Epoch 295/500\n",
      "2165/2165 [==============================] - 1s 660us/step - loss: 3.8748e-04 - val_loss: 6.2668e-04\n",
      "Epoch 296/500\n",
      "2165/2165 [==============================] - 1s 686us/step - loss: 3.8478e-04 - val_loss: 6.6931e-04\n",
      "Epoch 297/500\n",
      "2165/2165 [==============================] - 1s 660us/step - loss: 3.8576e-04 - val_loss: 6.1682e-04\n",
      "Epoch 298/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 3.7991e-04 - val_loss: 6.0519e-04\n",
      "Epoch 299/500\n",
      "2165/2165 [==============================] - 1s 653us/step - loss: 3.8515e-04 - val_loss: 6.3037e-04\n",
      "Epoch 300/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.6730e-04 - val_loss: 6.6682e-04\n",
      "Epoch 301/500\n",
      "2165/2165 [==============================] - 1s 654us/step - loss: 3.7270e-04 - val_loss: 6.0158e-04\n",
      "Epoch 302/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.8887e-04 - val_loss: 6.1682e-04\n",
      "Epoch 303/500\n",
      "2165/2165 [==============================] - 1s 658us/step - loss: 3.6238e-04 - val_loss: 6.4014e-04\n",
      "Epoch 304/500\n",
      "2165/2165 [==============================] - 2s 694us/step - loss: 3.6851e-04 - val_loss: 6.2703e-04\n",
      "Epoch 305/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 3.8783e-04 - val_loss: 6.1858e-04\n",
      "Epoch 306/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.6634e-04 - val_loss: 6.2186e-04\n",
      "Epoch 307/500\n",
      "2165/2165 [==============================] - 1s 658us/step - loss: 3.8368e-04 - val_loss: 6.3494e-04\n",
      "Epoch 308/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.7078e-04 - val_loss: 6.8209e-04\n",
      "Epoch 309/500\n",
      "2165/2165 [==============================] - 1s 650us/step - loss: 3.6626e-04 - val_loss: 6.3135e-04\n",
      "Epoch 310/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.8108e-04 - val_loss: 6.1214e-04\n",
      "Epoch 311/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 4.0314e-04 - val_loss: 6.1122e-04\n",
      "Epoch 312/500\n",
      "2165/2165 [==============================] - 1s 659us/step - loss: 3.7357e-04 - val_loss: 6.0600e-04\n",
      "Epoch 313/500\n",
      "2165/2165 [==============================] - 1s 656us/step - loss: 3.7039e-04 - val_loss: 6.0739e-04\n",
      "Epoch 314/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.7356e-04 - val_loss: 6.2299e-04\n",
      "Epoch 315/500\n",
      "2165/2165 [==============================] - 2s 701us/step - loss: 3.7090e-04 - val_loss: 6.0095e-04\n",
      "Epoch 316/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.6418e-04 - val_loss: 6.8151e-04\n",
      "Epoch 317/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 3.7659e-04 - val_loss: 6.4262e-04\n",
      "Epoch 318/500\n",
      "2165/2165 [==============================] - 1s 660us/step - loss: 3.8161e-04 - val_loss: 5.9981e-04\n",
      "Epoch 319/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 3.6286e-04 - val_loss: 6.0275e-04\n",
      "Epoch 320/500\n",
      "2165/2165 [==============================] - 1s 659us/step - loss: 3.6794e-04 - val_loss: 6.1610e-04\n",
      "Epoch 321/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.5403e-04 - val_loss: 6.1563e-04\n",
      "Epoch 322/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 3.8379e-04 - val_loss: 6.1000e-04\n",
      "Epoch 323/500\n",
      "2165/2165 [==============================] - 2s 693us/step - loss: 3.7751e-04 - val_loss: 6.2426e-04\n",
      "Epoch 324/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 3.6314e-04 - val_loss: 6.1804e-04\n",
      "Epoch 325/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 3.4677e-04 - val_loss: 5.7778e-04\n",
      "Epoch 326/500\n",
      "2165/2165 [==============================] - 2s 712us/step - loss: 3.6426e-04 - val_loss: 6.3062e-04\n",
      "Epoch 327/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.7293e-04 - val_loss: 6.5140e-04\n",
      "Epoch 328/500\n",
      "2165/2165 [==============================] - 1s 657us/step - loss: 3.6784e-04 - val_loss: 5.8096e-04\n",
      "Epoch 329/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.7948e-04 - val_loss: 6.3981e-04\n",
      "Epoch 330/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.5080e-04 - val_loss: 6.3186e-04\n",
      "Epoch 331/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.5649e-04 - val_loss: 6.2810e-04\n",
      "Epoch 332/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.6551e-04 - val_loss: 5.8131e-04\n",
      "Epoch 333/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.6873e-04 - val_loss: 6.1279e-04\n",
      "Epoch 334/500\n",
      "2165/2165 [==============================] - 1s 660us/step - loss: 3.3955e-04 - val_loss: 6.0363e-04\n",
      "Epoch 335/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.5907e-04 - val_loss: 6.1751e-04\n",
      "Epoch 336/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.5914e-04 - val_loss: 5.8287e-04\n",
      "Epoch 337/500\n",
      "2165/2165 [==============================] - 2s 704us/step - loss: 3.5515e-04 - val_loss: 5.9326e-04\n",
      "Epoch 338/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 3.5462e-04 - val_loss: 6.3734e-04\n",
      "Epoch 339/500\n",
      "2165/2165 [==============================] - 1s 658us/step - loss: 3.5022e-04 - val_loss: 6.0331e-04\n",
      "Epoch 340/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.6031e-04 - val_loss: 6.3235e-04\n",
      "Epoch 341/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.7333e-04 - val_loss: 6.1055e-04\n",
      "Epoch 342/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 3.6040e-04 - val_loss: 6.0587e-04\n",
      "Epoch 343/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.5165e-04 - val_loss: 6.1256e-04\n",
      "Epoch 344/500\n",
      "2165/2165 [==============================] - 1s 655us/step - loss: 3.4176e-04 - val_loss: 6.6331e-04\n",
      "Epoch 345/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.3307e-04 - val_loss: 6.5043e-04\n",
      "Epoch 346/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.5412e-04 - val_loss: 6.0219e-04\n",
      "Epoch 347/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.3576e-04 - val_loss: 5.8442e-04\n",
      "Epoch 348/500\n",
      "2165/2165 [==============================] - 2s 694us/step - loss: 3.4440e-04 - val_loss: 6.0896e-04\n",
      "Epoch 349/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.6798e-04 - val_loss: 5.8717e-04\n",
      "Epoch 350/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.4220e-04 - val_loss: 6.0842e-04\n",
      "Epoch 351/500\n",
      "2165/2165 [==============================] - 1s 676us/step - loss: 3.6199e-04 - val_loss: 6.1648e-04\n",
      "Epoch 352/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.4549e-04 - val_loss: 6.0364e-04\n",
      "Epoch 353/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.4215e-04 - val_loss: 6.1029e-04\n",
      "Epoch 354/500\n",
      "2165/2165 [==============================] - 1s 674us/step - loss: 3.5163e-04 - val_loss: 6.0441e-04\n",
      "Epoch 355/500\n",
      "2165/2165 [==============================] - 1s 682us/step - loss: 3.3920e-04 - val_loss: 6.1457e-04\n",
      "Epoch 356/500\n",
      "2165/2165 [==============================] - 1s 688us/step - loss: 3.4283e-04 - val_loss: 6.1209e-04\n",
      "Epoch 357/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 3.4324e-04 - val_loss: 5.8995e-04\n",
      "Epoch 358/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 3.3472e-04 - val_loss: 6.3766e-04\n",
      "Epoch 359/500\n",
      "2165/2165 [==============================] - 2s 703us/step - loss: 3.5142e-04 - val_loss: 6.1388e-04\n",
      "Epoch 360/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.2976e-04 - val_loss: 5.9983e-04\n",
      "Epoch 361/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.4533e-04 - val_loss: 5.9994e-04\n",
      "Epoch 362/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.3996e-04 - val_loss: 6.0060e-04\n",
      "Epoch 363/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 3.3726e-04 - val_loss: 6.0729e-04\n",
      "Epoch 364/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.6175e-04 - val_loss: 6.1603e-04\n",
      "Epoch 365/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 3.2499e-04 - val_loss: 6.2839e-04\n",
      "Epoch 366/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.3308e-04 - val_loss: 6.0198e-04\n",
      "Epoch 367/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 3.4015e-04 - val_loss: 6.2386e-04\n",
      "Epoch 368/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 3.3058e-04 - val_loss: 6.1543e-04\n",
      "Epoch 369/500\n",
      "2165/2165 [==============================] - 1s 657us/step - loss: 3.3613e-04 - val_loss: 6.0413e-04\n",
      "Epoch 370/500\n",
      "2165/2165 [==============================] - 2s 708us/step - loss: 3.3084e-04 - val_loss: 6.1133e-04\n",
      "Epoch 371/500\n",
      "2165/2165 [==============================] - 1s 659us/step - loss: 3.3784e-04 - val_loss: 6.2431e-04\n",
      "Epoch 372/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.1848e-04 - val_loss: 6.1580e-04\n",
      "Epoch 373/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.3120e-04 - val_loss: 5.9311e-04\n",
      "Epoch 374/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 3.6990e-04 - val_loss: 6.0051e-04\n",
      "Epoch 375/500\n",
      "2165/2165 [==============================] - 1s 660us/step - loss: 3.1769e-04 - val_loss: 6.0302e-04\n",
      "Epoch 376/500\n",
      "2165/2165 [==============================] - 1s 653us/step - loss: 3.2444e-04 - val_loss: 6.6893e-04\n",
      "Epoch 377/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.3101e-04 - val_loss: 6.2365e-04\n",
      "Epoch 378/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.3002e-04 - val_loss: 6.1306e-04\n",
      "Epoch 379/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.3656e-04 - val_loss: 6.1435e-04\n",
      "Epoch 380/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.4480e-04 - val_loss: 5.8018e-04\n",
      "Epoch 381/500\n",
      "2165/2165 [==============================] - 2s 705us/step - loss: 3.2759e-04 - val_loss: 5.9930e-04\n",
      "Epoch 382/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.2146e-04 - val_loss: 6.0656e-04\n",
      "Epoch 383/500\n",
      "2165/2165 [==============================] - 1s 659us/step - loss: 3.2453e-04 - val_loss: 6.1970e-04\n",
      "Epoch 384/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 3.2519e-04 - val_loss: 5.9489e-04\n",
      "Epoch 385/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 3.3220e-04 - val_loss: 5.9244e-04\n",
      "Epoch 386/500\n",
      "2165/2165 [==============================] - 1s 652us/step - loss: 3.3082e-04 - val_loss: 5.8833e-04\n",
      "Epoch 387/500\n",
      "2165/2165 [==============================] - 2s 696us/step - loss: 3.1877e-04 - val_loss: 5.8464e-04\n",
      "Epoch 388/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 3.2718e-04 - val_loss: 6.1280e-04\n",
      "Epoch 389/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.1475e-04 - val_loss: 6.1501e-04\n",
      "Epoch 390/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.3007e-04 - val_loss: 6.1767e-04\n",
      "Epoch 391/500\n",
      "2165/2165 [==============================] - 1s 675us/step - loss: 3.2783e-04 - val_loss: 6.0779e-04\n",
      "Epoch 392/500\n",
      "2165/2165 [==============================] - 2s 723us/step - loss: 3.3149e-04 - val_loss: 5.8126e-04\n",
      "Epoch 393/500\n",
      "2165/2165 [==============================] - 1s 677us/step - loss: 3.2409e-04 - val_loss: 6.0289e-04\n",
      "Epoch 394/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 3.1981e-04 - val_loss: 5.7624e-04\n",
      "Epoch 395/500\n",
      "2165/2165 [==============================] - 2s 693us/step - loss: 3.1769e-04 - val_loss: 5.9827e-04\n",
      "Epoch 396/500\n",
      "2165/2165 [==============================] - 1s 682us/step - loss: 3.2668e-04 - val_loss: 6.1354e-04\n",
      "Epoch 397/500\n",
      "2165/2165 [==============================] - 1s 683us/step - loss: 3.0573e-04 - val_loss: 6.4205e-04\n",
      "Epoch 398/500\n",
      "2165/2165 [==============================] - 1s 675us/step - loss: 3.1339e-04 - val_loss: 5.8474e-04\n",
      "Epoch 399/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 3.1531e-04 - val_loss: 5.9016e-04\n",
      "Epoch 400/500\n",
      "2165/2165 [==============================] - 2s 696us/step - loss: 3.2568e-04 - val_loss: 6.1578e-04\n",
      "Epoch 401/500\n",
      "2165/2165 [==============================] - 1s 681us/step - loss: 3.1533e-04 - val_loss: 6.0250e-04\n",
      "Epoch 402/500\n",
      "2165/2165 [==============================] - 2s 704us/step - loss: 3.2011e-04 - val_loss: 6.0514e-04\n",
      "Epoch 403/500\n",
      "2165/2165 [==============================] - 2s 728us/step - loss: 3.5009e-04 - val_loss: 5.7948e-04\n",
      "Epoch 404/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 3.2776e-04 - val_loss: 5.9352e-04\n",
      "Epoch 405/500\n",
      "2165/2165 [==============================] - 1s 679us/step - loss: 3.2807e-04 - val_loss: 5.7493e-04\n",
      "Epoch 406/500\n",
      "2165/2165 [==============================] - 2s 708us/step - loss: 3.0968e-04 - val_loss: 5.8443e-04\n",
      "Epoch 407/500\n",
      "2165/2165 [==============================] - 1s 687us/step - loss: 3.1717e-04 - val_loss: 6.1373e-04\n",
      "Epoch 408/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 3.1979e-04 - val_loss: 5.9385e-04\n",
      "Epoch 409/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 3.1111e-04 - val_loss: 5.7813e-04\n",
      "Epoch 410/500\n",
      "2165/2165 [==============================] - 1s 680us/step - loss: 3.2292e-04 - val_loss: 5.8865e-04\n",
      "Epoch 411/500\n",
      "2165/2165 [==============================] - 1s 675us/step - loss: 3.0338e-04 - val_loss: 6.4739e-04\n",
      "Epoch 412/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.1511e-04 - val_loss: 5.9304e-04\n",
      "Epoch 413/500\n",
      "2165/2165 [==============================] - 1s 677us/step - loss: 3.3167e-04 - val_loss: 5.9894e-04\n",
      "Epoch 414/500\n",
      "2165/2165 [==============================] - 2s 710us/step - loss: 3.2739e-04 - val_loss: 6.3484e-04\n",
      "Epoch 415/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.1323e-04 - val_loss: 5.9466e-04\n",
      "Epoch 416/500\n",
      "2165/2165 [==============================] - 1s 678us/step - loss: 3.2278e-04 - val_loss: 5.8492e-04\n",
      "Epoch 417/500\n",
      "2165/2165 [==============================] - 1s 691us/step - loss: 3.1120e-04 - val_loss: 6.2353e-04\n",
      "Epoch 418/500\n",
      "2165/2165 [==============================] - 1s 675us/step - loss: 3.0628e-04 - val_loss: 6.0848e-04\n",
      "Epoch 419/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.1127e-04 - val_loss: 6.1029e-04\n",
      "Epoch 420/500\n",
      "2165/2165 [==============================] - 1s 690us/step - loss: 3.0432e-04 - val_loss: 6.0048e-04\n",
      "Epoch 421/500\n",
      "2165/2165 [==============================] - 1s 688us/step - loss: 3.1031e-04 - val_loss: 5.9840e-04\n",
      "Epoch 422/500\n",
      "2165/2165 [==============================] - 2s 733us/step - loss: 3.0257e-04 - val_loss: 5.8354e-04\n",
      "Epoch 423/500\n",
      "2165/2165 [==============================] - 1s 673us/step - loss: 3.1464e-04 - val_loss: 5.8273e-04\n",
      "Epoch 424/500\n",
      "2165/2165 [==============================] - 2s 708us/step - loss: 3.1486e-04 - val_loss: 5.9107e-04\n",
      "Epoch 425/500\n",
      "2165/2165 [==============================] - 1s 673us/step - loss: 3.1126e-04 - val_loss: 6.1075e-04\n",
      "Epoch 426/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 3.0444e-04 - val_loss: 6.0677e-04\n",
      "Epoch 427/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 3.1842e-04 - val_loss: 5.8234e-04\n",
      "Epoch 428/500\n",
      "2165/2165 [==============================] - 1s 680us/step - loss: 3.1233e-04 - val_loss: 5.9546e-04\n",
      "Epoch 429/500\n",
      "2165/2165 [==============================] - 1s 677us/step - loss: 3.0856e-04 - val_loss: 5.8743e-04\n",
      "Epoch 430/500\n",
      "2165/2165 [==============================] - 1s 674us/step - loss: 3.1605e-04 - val_loss: 5.7514e-04\n",
      "Epoch 431/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165/2165 [==============================] - 1s 665us/step - loss: 2.9568e-04 - val_loss: 5.7167e-04\n",
      "Epoch 432/500\n",
      "2165/2165 [==============================] - 1s 679us/step - loss: 3.2349e-04 - val_loss: 6.0374e-04\n",
      "Epoch 433/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 3.0141e-04 - val_loss: 5.9052e-04\n",
      "Epoch 434/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 3.1073e-04 - val_loss: 5.8636e-04\n",
      "Epoch 435/500\n",
      "2165/2165 [==============================] - 2s 708us/step - loss: 3.0485e-04 - val_loss: 6.0525e-04\n",
      "Epoch 436/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 3.0425e-04 - val_loss: 5.6200e-04\n",
      "Epoch 437/500\n",
      "2165/2165 [==============================] - 1s 675us/step - loss: 3.1192e-04 - val_loss: 6.0799e-04\n",
      "Epoch 438/500\n",
      "2165/2165 [==============================] - 2s 703us/step - loss: 3.0440e-04 - val_loss: 5.8473e-04\n",
      "Epoch 439/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 2.9755e-04 - val_loss: 5.7923e-04\n",
      "Epoch 440/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.1298e-04 - val_loss: 5.7770e-04\n",
      "Epoch 441/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 2.9435e-04 - val_loss: 5.7983e-04\n",
      "Epoch 442/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 2.9413e-04 - val_loss: 5.9128e-04\n",
      "Epoch 443/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.0061e-04 - val_loss: 5.8086e-04\n",
      "Epoch 444/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 3.1199e-04 - val_loss: 6.0245e-04\n",
      "Epoch 445/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 3.0433e-04 - val_loss: 5.8268e-04\n",
      "Epoch 446/500\n",
      "2165/2165 [==============================] - 2s 701us/step - loss: 2.9555e-04 - val_loss: 6.1910e-04\n",
      "Epoch 447/500\n",
      "2165/2165 [==============================] - 1s 664us/step - loss: 2.9925e-04 - val_loss: 6.1806e-04\n",
      "Epoch 448/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 2.9810e-04 - val_loss: 5.8021e-04\n",
      "Epoch 449/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 2.8741e-04 - val_loss: 6.2086e-04\n",
      "Epoch 450/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 2.9280e-04 - val_loss: 6.3947e-04\n",
      "Epoch 451/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 2.9890e-04 - val_loss: 5.9644e-04\n",
      "Epoch 452/500\n",
      "2165/2165 [==============================] - 1s 661us/step - loss: 3.0084e-04 - val_loss: 5.6423e-04\n",
      "Epoch 453/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 3.0289e-04 - val_loss: 5.8094e-04\n",
      "Epoch 454/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 3.0272e-04 - val_loss: 6.0208e-04\n",
      "Epoch 455/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 2.9217e-04 - val_loss: 6.0371e-04\n",
      "Epoch 456/500\n",
      "2165/2165 [==============================] - 1s 674us/step - loss: 2.8634e-04 - val_loss: 5.7171e-04\n",
      "Epoch 457/500\n",
      "2165/2165 [==============================] - 2s 707us/step - loss: 2.9781e-04 - val_loss: 5.9219e-04\n",
      "Epoch 458/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 3.2115e-04 - val_loss: 5.8704e-04\n",
      "Epoch 459/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 3.0762e-04 - val_loss: 6.0221e-04\n",
      "Epoch 460/500\n",
      "2165/2165 [==============================] - 1s 684us/step - loss: 3.0712e-04 - val_loss: 5.7129e-04\n",
      "Epoch 461/500\n",
      "2165/2165 [==============================] - 1s 685us/step - loss: 2.8970e-04 - val_loss: 5.5096e-04\n",
      "Epoch 462/500\n",
      "2165/2165 [==============================] - 2s 701us/step - loss: 2.9278e-04 - val_loss: 5.7639e-04\n",
      "Epoch 463/500\n",
      "2165/2165 [==============================] - 2s 701us/step - loss: 2.8656e-04 - val_loss: 6.3955e-04\n",
      "Epoch 464/500\n",
      "2165/2165 [==============================] - 1s 691us/step - loss: 2.9069e-04 - val_loss: 6.0199e-04\n",
      "Epoch 465/500\n",
      "2165/2165 [==============================] - 1s 684us/step - loss: 2.8729e-04 - val_loss: 6.2052e-04\n",
      "Epoch 466/500\n",
      "2165/2165 [==============================] - 1s 676us/step - loss: 2.8607e-04 - val_loss: 5.9525e-04\n",
      "Epoch 467/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 2.9577e-04 - val_loss: 5.7267e-04\n",
      "Epoch 468/500\n",
      "2165/2165 [==============================] - 2s 705us/step - loss: 2.8601e-04 - val_loss: 5.9683e-04\n",
      "Epoch 469/500\n",
      "2165/2165 [==============================] - 1s 677us/step - loss: 3.0506e-04 - val_loss: 5.8405e-04\n",
      "Epoch 470/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 2.8387e-04 - val_loss: 5.9599e-04\n",
      "Epoch 471/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 2.8412e-04 - val_loss: 5.8397e-04\n",
      "Epoch 472/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 2.9013e-04 - val_loss: 5.7756e-04\n",
      "Epoch 473/500\n",
      "2165/2165 [==============================] - 1s 668us/step - loss: 2.9925e-04 - val_loss: 6.0074e-04\n",
      "Epoch 474/500\n",
      "2165/2165 [==============================] - 1s 663us/step - loss: 2.7259e-04 - val_loss: 5.9201e-04\n",
      "Epoch 475/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 2.9756e-04 - val_loss: 5.9271e-04\n",
      "Epoch 476/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 3.1018e-04 - val_loss: 5.8824e-04\n",
      "Epoch 477/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 2.8704e-04 - val_loss: 5.7678e-04\n",
      "Epoch 478/500\n",
      "2165/2165 [==============================] - 1s 674us/step - loss: 2.8651e-04 - val_loss: 5.8502e-04\n",
      "Epoch 479/500\n",
      "2165/2165 [==============================] - 2s 707us/step - loss: 2.8649e-04 - val_loss: 5.6747e-04\n",
      "Epoch 480/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 2.9121e-04 - val_loss: 6.0034e-04\n",
      "Epoch 481/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 2.9850e-04 - val_loss: 5.9708e-04\n",
      "Epoch 482/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 2.9244e-04 - val_loss: 5.8925e-04\n",
      "Epoch 483/500\n",
      "2165/2165 [==============================] - 1s 670us/step - loss: 2.8491e-04 - val_loss: 5.6548e-04\n",
      "Epoch 484/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 2.9966e-04 - val_loss: 5.6091e-04\n",
      "Epoch 485/500\n",
      "2165/2165 [==============================] - 1s 665us/step - loss: 2.8638e-04 - val_loss: 6.0006e-04\n",
      "Epoch 486/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 2.8302e-04 - val_loss: 5.7975e-04\n",
      "Epoch 487/500\n",
      "2165/2165 [==============================] - 1s 675us/step - loss: 2.7400e-04 - val_loss: 5.7130e-04\n",
      "Epoch 488/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 2.8594e-04 - val_loss: 5.9488e-04\n",
      "Epoch 489/500\n",
      "2165/2165 [==============================] - 1s 671us/step - loss: 2.8122e-04 - val_loss: 5.4922e-04\n",
      "Epoch 490/500\n",
      "2165/2165 [==============================] - 2s 706us/step - loss: 2.8701e-04 - val_loss: 5.8088e-04\n",
      "Epoch 491/500\n",
      "2165/2165 [==============================] - 1s 672us/step - loss: 2.9947e-04 - val_loss: 5.9808e-04\n",
      "Epoch 492/500\n",
      "2165/2165 [==============================] - 1s 673us/step - loss: 2.7841e-04 - val_loss: 5.6309e-04\n",
      "Epoch 493/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 2.7971e-04 - val_loss: 5.7065e-04\n",
      "Epoch 494/500\n",
      "2165/2165 [==============================] - 1s 669us/step - loss: 2.7926e-04 - val_loss: 5.9434e-04\n",
      "Epoch 495/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 2.7487e-04 - val_loss: 5.9313e-04\n",
      "Epoch 496/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 2.8795e-04 - val_loss: 5.7638e-04\n",
      "Epoch 497/500\n",
      "2165/2165 [==============================] - 1s 662us/step - loss: 2.8468e-04 - val_loss: 5.8108e-04\n",
      "Epoch 498/500\n",
      "2165/2165 [==============================] - 1s 677us/step - loss: 2.7620e-04 - val_loss: 5.8409e-04\n",
      "Epoch 499/500\n",
      "2165/2165 [==============================] - 1s 667us/step - loss: 2.8288e-04 - val_loss: 5.7061e-04\n",
      "Epoch 500/500\n",
      "2165/2165 [==============================] - 1s 666us/step - loss: 2.6591e-04 - val_loss: 5.6624e-04\n"
     ]
    }
   ],
   "source": [
    "# build the model \n",
    "\n",
    "# some old code commented out \n",
    "# from tensorflow.python.keras.utils.data_utils import Sequence \n",
    "# model.fit(x_batch,y_batch, epochs=1, steps_per_epoch=5, validation_data=validation_data)\n",
    "-\n",
    "# model.fit(x_batch,y_batch,\n",
    "#           epochs=200,batch_size=16, validation_split=0.1, verbose=1)  #this is for the 60 day sequence. \n",
    "\n",
    "# history_fert_dayssince_4cov_lr001_rmsprop_lstm_dro20 = model.fit(x_data,y_data,\n",
    "#          epochs=500,batch_size=16, validation_split=0.1, verbose=1, callbacks = callbacks)\n",
    "\n",
    "\n",
    "\n",
    "lstm_mod = model.fit(x_data,y_data,sample_weight=sample_weight,\n",
    "          epochs=500,batch_size=16, validation_split=0.1, verbose=1, callbacks = callbacks)\n",
    "\n",
    "# is training/building (calibration) and testing (validation) done? \n",
    "    # holdout happens in the next script (estimation), but calibration/validation (train/test) should be done at the end of this script...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
